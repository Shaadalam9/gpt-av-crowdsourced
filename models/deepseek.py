import os
import json
import pandas as pd
import torch
import common
from transformers import AutoModelForCausalLM  # noqa:F401
from deepseek_vl2.models import DeepseekVLV2Processor, DeepseekVLV2ForCausalLM
from deepseek_vl2.utils.io import load_pil_images
from langchain.memory import ConversationBufferMemory
from langchain.schema import messages_from_dict, messages_to_dict
from custom_logger import CustomLogger
from logmod import logs

logs(show_level='info', show_color=True)
logger = CustomLogger(__name__)  # use custom logger

# Disable parallelism in tokenizer warnings for smoother execution.
os.environ["TOKENIZERS_PARALLELISM"] = "false"


class VisualQuestionAnswering:
    """
    A class that enables visual question answering using DeepSeek VL2.

    This class supports contextual conversations by maintaining a conversation memory.
    It loads a model and processor, encodes image inputs, processes user questions,
    generates responses, and logs the results in a CSV file.
    """

    def __init__(self, model_path="deepseek-ai/deepseek-vl2-tiny", use_history=True, max_memory_messages=6):
        """
        Initialise the VisualQuestionAnswering instance.

        Args:
            model_path (str): Path or identifier for the DeepSeek VL2 model.
            use_history (bool): Flag to indicate whether to include conversation history in prompts.
            max_memory_messages (int): Maximum number of conversation history messages to store.
        """
        self.model_path = model_path
        # Determine device based on availability of CUDA.
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.use_history = use_history
        self.first_run = True  # Flag to indicate if this is the first run (used for history handling).
        self.max_memory_messages = max_memory_messages
        # File to store conversation memory.
        self.memory_file = os.path.join(common.get_configs("output"), "deepseek_memory.json")
        # Initialise conversation memory using LangChain.
        self.memory = ConversationBufferMemory(return_messages=True)
        self.load_memory()
        # Load model and processor.
        self._load_model_and_processor()

    def load_memory(self):
        """
        Load conversation memory from a JSON file.

        The stored memory is loaded and truncated to the most recent `max_memory_messages`.
        If the memory file doesn't exist, it simply continues without loading.
        """
        try:
            with open(self.memory_file, "r") as f:
                messages = json.load(f)
                full_list = messages_from_dict(messages)
                # Retain only the last max_memory_messages for context.
                self.memory.chat_memory.messages = full_list[-self.max_memory_messages:]
        except FileNotFoundError:
            # No memory file found; start with an empty history.
            pass

    def save_memory(self):
        """
        Save the current conversation memory to a JSON file.

        The conversation history is serialised to a dictionary format and written to disk.
        """
        messages = messages_to_dict(self.memory.chat_memory.messages)
        with open(self.memory_file, "w") as f:
            json.dump(messages, f, indent=2)

    def _load_model_and_processor(self):
        """
        Load the DeepSeek VL2 model and processor.

        This method loads the processor (for image and text inputs) and the causal language model,
        then moves the model to the appropriate device and sets it to evaluation mode.
        """
        self.processor = DeepseekVLV2Processor.from_pretrained(self.model_path)
        self.tokenizer = self.processor.tokenizer  # type: ignore

        self.model = DeepseekVLV2ForCausalLM.from_pretrained(
            self.model_path, trust_remote_code=True
        )
        # Convert model to bfloat16 if supported, move to the device, and set evaluation mode.
        self.model = self.model.to(torch.bfloat16).to(self.device).eval()  # type: ignore

    def ask_question(self, image_path, question, model_name="deepseek-vl2", seed=42):
        """
        Process a visual question by sending the image and prompt to the model and update the results.

        This method prepares the input by incorporating conversation history (if enabled),
        loads the image, processes it with the model, decodes the generated output,
        updates the conversation memory, and logs the response in a CSV file.

        Args:
            image_path (str): Path to the image file.
            question (str): The question or prompt to ask about the image.
            model_name (str): Name of the model column to store the answer in the CSV file.
            seed (int): A seed value for reproducibility and naming the output CSV.

        Returns:
            str: The answer generated by the model.
        """
        # Set the output CSV file based on the provided seed.
        output_csv = os.path.join(common.get_configs("output"), f"output_{seed}.csv")

        # Build the prompt with conversation history if enabled and not the first run.
        if self.use_history and not self.first_run:
            formatted_history = ""
            # Iterate through stored conversation messages.
            for message in self.memory.chat_memory.messages:
                if message.__class__.__name__ == "HumanMessage":
                    formatted_history += f"History - Human: {message.content}\n"
                elif message.__class__.__name__ == "AIMessage":
                    formatted_history += f"History - AI: {message.content}\n"

            full_prompt = (
                f"{common.get_configs('base_prompt')}\n\n"
                f"{common.get_configs('history_intro')}\n"
                f"{formatted_history}\n"
                f"{common.get_configs('current_image_instruction')}"
            )
        else:
            full_prompt = question

        # Create a conversation structure that includes a placeholder for image and prompt.
        conversation = [
            {
                "role": "<|User|>",
                "content": f"<image>\n{full_prompt}",
                "images": [image_path],
            },
            {"role": "<|Assistant|>", "content": ""}
        ]

        # Load the image(s) using the processor utility.
        pil_images = load_pil_images(conversation)
        # Process the conversation and images to generate model inputs.
        inputs = self.processor(
            conversations=conversation,
            images=pil_images,
            force_batchify=True,
            system_prompt=""
        ).to(self.model.device)  # type: ignore

        # Prepare the embeddings required for generation.
        inputs_embeds = self.model.prepare_inputs_embeds(**inputs)

        # Generate output tokens from the model.
        outputs = self.model.generate(
            inputs_embeds=inputs_embeds,
            attention_mask=inputs.attention_mask,
            pad_token_id=self.tokenizer.eos_token_id,
            bos_token_id=self.tokenizer.bos_token_id,
            eos_token_id=self.tokenizer.eos_token_id,
            max_new_tokens=512,
            do_sample=False,
            use_cache=True
        )

        # Decode the generated tokens into a text string.
        decoded = self.tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)
        # Extract the assistant's answer by splitting on the assistant role marker.
        answer = decoded.split("<|Assistant|>")[-1].strip()

        # If using conversation history, update memory with the new messages.
        if self.use_history:
            self.memory.chat_memory.add_user_message(question)
            self.memory.chat_memory.add_ai_message(answer)
            # Limit history to the most recent max_memory_messages.
            self.memory.chat_memory.messages = self.memory.chat_memory.messages[-self.max_memory_messages:]
            self.save_memory()

        # Mark that the first run has been completed.
        self.first_run = False

        image_name = os.path.basename(image_path)
        # Try to load an existing CSV file with prior outputs.
        try:
            df = pd.read_csv(output_csv)
        except FileNotFoundError:
            # If the CSV does not exist, create a new DataFrame with an "image" column.
            df = pd.DataFrame(columns=["image"])

        # Ensure the column for this model exists in the DataFrame.
        if model_name not in df.columns:
            df[model_name] = pd.NA

        # Update the row corresponding to the current image or add a new row.
        if image_name in df["image"].values:
            df.loc[df["image"] == image_name, model_name] = answer
        else:
            new_row = {"image": image_name, model_name: answer}
            df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)

        # Save the updated DataFrame to the CSV file.
        df.to_csv(output_csv, index=False)
        logger.info(f"\nSaved DeepSeek-VL2 output for {image_name} to {output_csv}")
        return answer


if __name__ == "__main__":
    # The main block is left empty for now.
    pass
